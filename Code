from urllib.request import urlopen, Request

import requests
from bs4 import BeautifulSoup
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta


coin_desk_url = 'https://www.coindesk.com/tag/'
coin_telegraph_url = 'https://cointelegraph.com/tags/'
news_btc_url = "https://www.newsbtc.com/news/"
crypto_briefing_url = 'https://cryptobriefing.com/?s='
current_datetime = datetime.now()
yesterday = (datetime.now()-timedelta(days=1)).strftime('%Y-%m-%d')
tickers = ['bitcoin','ethereum']
page_numbers = [str(x) for x in list((range(1,25)))]
parsed_data = []

# Webscrape for coin_desk
for ticker in tickers:
    for page in page_numbers:
        url = coin_desk_url + ticker + "/" + page
        req = Request(url=url, headers={'user-agent': 'my-app'})
        response = urlopen(req)

        html = BeautifulSoup(response, 'html.parser')
        articles = html.find_all("div", class_='article-cardstyles__AcTitle-sc-q1x8lc-1 PUjAZ articleTextSection')
        for article in articles:
            title = article.find("a", class_="card-title").text
            content = article.find("span", class_="content-text").text
            timestamp = article.find("span", class_="typography__StyledTypography-sc-owin6q-0 hcIsFR").text
            date_data = timestamp.split(' ')
            date = date_data[0] + ', ' + date_data[1] + ' ' + date_data[2]
            time = date_data[4]+ ' ' + date_data[5]
            parsed_data.append([ticker, date, title, content])

# coin_telegraph web scrape
for ticker in tickers:
    url = coin_telegraph_url + ticker

    req = Request(url=url, headers={'user-agent': 'my-app'})
    response = urlopen(req)

    html = BeautifulSoup(response, 'html.parser')
    articles = html.find_all("div", class_='post-card-inline__content')
    for article in articles:
        title = article.find("span", class_="post-card-inline__title").text
        content = article.find("p", class_="post-card-inline__text").text
        date_element = article.find("time", class_="post-card-inline__date")
        date = date_element.get("datetime") if date_element else None
        parsed_data.append([ticker, date, title, content])

# NewsBtc scraper
for ticker in tickers:
    url = news_btc_url + ticker

    req = Request(url=url, headers={'user-agent': 'my-app'})
    response = urlopen(req)

    html = BeautifulSoup(response, 'html.parser')
    articles = html.find_all("article", class_='jeg_post jeg_pl_md_2 format-standard')
    for article in articles:
        title = article.find("h3", class_="jeg_post_title").text
        content = article.find("div", class_="jeg_post_excerpt").text
        days_ago = article.find("div", class_="jeg_meta_date").text
        time_split = days_ago.split(" ")
        ago = int(time_split[1])
        if time_split[2] == "hour" or time_split[2] == "hours":
            date = current_datetime.date()
        elif time_split[2] == "day" or time_split[2] == "days":
            time_ago = timedelta(days=ago)
            time_date = current_datetime - time_ago
            date = time_date.date()
        elif time_split[2] == "week" or time_split[2] == "weeks":
            days_ago = ago * 7
            time_ago = timedelta(days=days_ago)
            time_date = current_datetime - time_ago
            date = time_date.date()
        elif time_split[2] == "month" or time_split[2] == "months":
            days_ago = ago * 31
            time_ago = timedelta(days=days_ago)
            time_date = current_datetime - time_ago
            date = time_date.date()
        parsed_data.append([ticker, date, title, content])

# Crypto Briefing scraper
for ticker in tickers:
    url = crypto_briefing_url + ticker

    req = Request(url=url, headers={'user-agent': 'my-app'})
    response = urlopen(req)

    html = BeautifulSoup(response, 'html.parser')
    articles = html.find_all("section", class_='main-news-content')
    for article in articles:
        title = article.find("h2", class_="main-news-title").text
        content = article.find("p", class_="main-news-message").text
        date = article.find("time").text
        parsed_data.append([ticker, date, title, content])

for ticker in tickers:
    url = 'https://news.google.com/search?q={ticker}&hl=en-US&gl=US&ceid=US%3Aen'

    req = Request(url=url, headers={'user-agent': 'my-app'})
    response = urlopen(req)

    html = BeautifulSoup(response, 'html.parser')
    articles = html.find_all("article", class_='MQsxIb xTewfe R7GTQ keNKEd j7vNaf Cc0Z5d VkAdve GU7x0c JMJvke q4atFc')
    for article in articles:
        title = article.find("a", class_="DY5T1d RZIKme").text
        content = article.find("a", class_="DY5T1d RZIKme").text
        date = article.find("time").get("datetime")
        parsed_data.append([ticker, date, title, content])

df = pd.DataFrame(parsed_data, columns=['ticker', 'date', 'title', 'content'])
print(f'There is a total of {df.shape[0]} articles')
vader = SentimentIntensityAnalyzer()

f = lambda title: vader.polarity_scores(title)['compound']
df['title-compound'] = df['title'].apply(f)
df['content-compound'] = df['content'].apply(f)
df['date'] = pd.to_datetime(df.date,format="mixed").dt.date

mean_df = df.groupby(['ticker', 'date']).mean(numeric_only=True)
print(mean_df)

for ticker in tickers:

    ticker_data = mean_df.xs(ticker)
    ticker_data.plot(kind="bar")
    plt.gcf().set_size_inches(16, 8)

    index = tickers.index(ticker)
    start_date = ticker_data.index.min() - timedelta(days=1)
    end_date = ticker_data.index.max()
    prices = yf.download(yf_ticker[index], start=start_date, end=end_date)
    adj_close = prices['Adj Close']
    returns = adj_close / adj_close.shift(1)
    print(returns)

    print(ticker_data)
    plt.title(f'Bar Plot for Ticker: {ticker}')
    plt.xlabel('Date')
    plt.ylabel('Mean Value')
    # ticker_data = ticker_data.xs('title-compound', axis="columns").transpose()
plt.show()

"""
# News API scraper:
for ticker in tickers:
    url = ('https://newsapi.org/v2/everything?'
           'q={ticker}&'
           'from={yesterday}&'
           'sortBy=popularity&'
           'apiKey=506917b7fb0f4a99b0798d79e17ddd6b')

    response = requests.get(url=url)
    data = response.json()
    articles = data['articles']
    for article in articles:
        title = article['title']
        content = article['content']
        published_at = article['publishedAt']
        if published_at != None:
            time_split = published_at.split("T")
            date = time_split[0]
        else:
            continue
        parsed_data.append([ticker, date, title, content])
"""
"""
# Cryptoslate webscraper
for ticker in tickers:
    url = crypto_slate_url + ticker

    req = Request(url=url, headers={'user-agent': 'my-app'})
    response = urlopen(req)

    html = BeautifulSoup(response, 'html.parser')
    articles = html.find_all("div", class_="list-post")
    print(articles)
    for article in articles:
        title = article.find("h2").text
        print(title)
        excerpt_div = article.find("div", class_='excerpt')
        if excerpt_div:
            content = excerpt_div.text
            print("Content:", content)
        else:
            content = title
            print("Content: No excerpt available, Using Title")
        days_ago = article.find("span", class_="post-meta").text
        time_split = days_ago.split(" ")
        print(time_split)
        ago = int(time_split[1])
        if time_split[2] == "hour" or time_split[2] == "hours":
            date = current_datetime.date()
            print(date)
        elif time_split[2] == "day" or time_split[2] == "days":
            time_ago = timedelta(days=ago)
            time_date = current_datetime - time_ago
            date = time_date.date()
            print(date)
        elif time_split[2] == "week" or time_split[2] == "weeks":
            days_ago = ago * 7
            time_ago = timedelta(days=days_ago)
            time_date = current_datetime - time_ago
            date = time_date.date()
            print(date)
        elif time_split[2] == "month" or time_split[2] == "months":
            days_ago = ago * 31
            time_ago = timedelta(days=days_ago)
            time_date = current_datetime - time_ago
            date = time_date.date()
            print(date)
        parsed_data.append([ticker, date, title, content])
"""

